# Par√¢metros de Infer√™ncia em LLMs: Uma Explora√ß√£o Estoc√°stica

Este reposit√≥rio apresenta um estudo detalhado sobre os mecanismos que controlam a gera√ß√£o de texto em Grandes Modelos de Linguagem (LLMs). O projeto explora a transi√ß√£o dos modelos de uma decodifica√ß√£o puramente determin√≠stica para amostragens probabil√≠sticas, analisando como a manipula√ß√£o de **Logits** e **Probabilidades** influencia a criatividade e a coer√™ncia das sa√≠das.

## üöÄ Ambiente de Execu√ß√£o

> [!IMPORTANT]
> **Este projeto foi desenvolvido e otimizado para o Google Colab.**
> Devido ao uso intensivo de tensores com PyTorch e √† necessidade de acelera√ß√£o por GPU para a infer√™ncia do modelo **Phi-2**, recomenda-se fortemente a execu√ß√£o atrav√©s do ambiente Colab para garantir a compatibilidade de bibliotecas e recursos de hardware.

## üìö Conte√∫do do Estudo

O notebook est√° organizado de forma did√°tica, integrando teoria estat√≠stica, implementa√ß√µes em Python/PyTorch e visualiza√ß√µes gr√°ficas:

* **Fundamentos**: Convers√£o de Logits em Probabilidades via fun√ß√£o Softmax.
* **Escalonamento de Temperatura ()**: Modula√ß√£o da entropia da distribui√ß√£o para controle de criatividade.
* **Amostragem Top-K**: Restri√ß√£o do espa√ßo amostral aos  tokens mais prov√°veis.
* **Amostragem Top-P (Nucleus Sampling)**: Filtragem din√¢mica baseada na massa de probabilidade acumulada.
* **Demonstra√ß√£o Pr√°tica**: Testes de infer√™ncia utilizando o modelo **Phi-2** da Microsoft, explorando o impacto real de cada par√¢metro na gera√ß√£o de texto.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem**: Python 3.12+
* **Framework**: PyTorch (Manipula√ß√£o de Tensores)
* **Modelos**: Microsoft Phi-2 via Hugging Face Transformers
* **Visualiza√ß√£o**: Matplotlib e Pandas

## üìñ Refer√™ncias

Este trabalho utiliza como base principal o livro **"Build a Large Language Model (From Scratch)"** de Sebastian Raschka e as implementa√ß√µes de refer√™ncia da biblioteca **Transformers** da Hugging Face. Abaixo est√£o listadas as refer√™ncias bibliogr√°ficas e tecnol√≥gicas que fundamentaram o desenvolvimento deste notebook e a implementa√ß√£o dos par√¢metros de infer√™ncia:

* **HOLTZMAN, Ari et al.** The Curious Case of Neural Text Degeneration. *In: International Conference on Learning Representations (ICLR)*, 2020. Dispon√≠vel em: https://arxiv.org/abs/1904.09751. (Refer√™ncia principal para a t√©cnica de **Amostragem Top-P**).

* **HUGGING FACE.** Transformers Documentation: LogitsProcessor. Dispon√≠vel em: https://huggingface.co/docs/transformers/main/en/internal/generation_utils. (Base para a implementa√ß√£o das fun√ß√µes de amostragem e uso do modelo **Phi-2**).

* **HUGGING FACE - Phi-2.** Transformers Documentation: **Phi-2**. Dispon√≠vel em: https://huggingface.co/microsoft/phi-2.

* **MICROSOFT.** Phi-2: The surprising power of small language models. *Microsoft Research Blog*, 2023. Dispon√≠vel em: https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/. (Refer√™ncia do modelo utilizado na demonstra√ß√£o pr√°tica).

* **RASCHKA, Sebastian.** *Build a Large Language Model (From Scratch)*. Shelter Island, NY: Manning Publications, 2024. (Refer√™ncia para os fundamentos de arquitetura de LLMs e estrat√©gias de decodifica√ß√£o).

* **PYTORCH.**. Dispon√≠vel em: https://pytorch.org/docs/stable/tensors.html. (Refer√™ncia t√©cnica para as opera√ß√µes de tensores utilizadas nas m√°scaras de logits).
